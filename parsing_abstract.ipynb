{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f8a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 22/126 [00:01<00:07, 14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore durante la lettura di /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/18_Computer aided robotics for applications in fracture reduction surgery advances, challenges and opportunities.pdf: Cannot open empty file: filename='/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/18_Computer aided robotics for applications in fracture reduction surgery advances, challenges and opportunities.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 122/126 [00:12<00:00, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore durante la lettura di /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/48_Metaverse-based simulation  a scoping review of charting medical education over the last two decades in the lens of the  marvelous medical education m.pdf: Cannot open empty file: filename='/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/48_Metaverse-based simulation  a scoping review of charting medical education over the last two decades in the lens of the  marvelous medical education m.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:12<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  \\\n",
      "0  53_Symbiotic relationship between robotics and...   \n",
      "1  60_What is a digital twin experimental design ...   \n",
      "2  2_A deep learning approach to estimate xâ€ray s...   \n",
      "3  22_Digital twin current scenario and a case st...   \n",
      "4  54_Revolutionizing Healthcare A Review Unveili...   \n",
      "\n",
      "                                               title  \\\n",
      "0                     Symbiotic relationship between   \n",
      "1                                          \u0001\u0002\u0003\u0001\u0004\u0005\u0006   \n",
      "2                          Received: 8 February 2023   \n",
      "3                                   ORIGINAL ARTICLE   \n",
      "4  Received 20 March 2024, accepted 3 May 2024, d...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Purpose â€“ The present paper aims to demonstrat...  \n",
      "1  been used predominantly for problems in engine...  \n",
      "2  Background: Digital breast tomosynthesis (DBT)...  \n",
      "3  In the current scenario, industries need to ha...  \n",
      "4  transformative force, holding the promise of r...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_title_and_abstract_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "\n",
    "        lines = text.split(\"\\n\")\n",
    "        lines = [line.strip() for line in lines if line.strip() != \"\"]\n",
    "\n",
    "        title = lines[0] if lines else \"\"\n",
    "\n",
    "        abstract = \"\"\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"abstract\" in line.lower():\n",
    "                abstract_lines = lines[i+1:i+6]\n",
    "                abstract = \" \".join(abstract_lines)\n",
    "                break\n",
    "\n",
    "        return title, abstract\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la lettura di {pdf_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def scan_papers_in_folder(folder_path):\n",
    "    data = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            title, abstract = extract_title_and_abstract_from_pdf(full_path)\n",
    "            if title and abstract:\n",
    "                data.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Imposta il path alla tua cartella di PDF\n",
    "folder_path = \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged\"\n",
    "\n",
    "# Esegui lo scan\n",
    "df_papers = scan_papers_in_folder(folder_path)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(df_papers.head())\n",
    "\n",
    "# Salva in un file CSV (opzionale)\n",
    "df_papers.to_csv(\"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/index.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a3acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 22/126 [00:01<00:07, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/18_Computer aided robotics for applications in fracture reduction surgery advances, challenges and opportunities.pdf: Cannot open empty file: filename='/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/18_Computer aided robotics for applications in fracture reduction surgery advances, challenges and opportunities.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 122/126 [00:12<00:00, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/48_Metaverse-based simulation  a scoping review of charting medical education over the last two decades in the lens of the  marvelous medical education m.pdf: Cannot open empty file: filename='/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged/48_Metaverse-based simulation  a scoping review of charting medical education over the last two decades in the lens of the  marvelous medical education m.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:12<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  53_Symbiotic relationship between robotics and...   \n",
      "1  60_What is a digital twin experimental design ...   \n",
      "2  2_A deep learning approach to estimate xâ€ray s...   \n",
      "3  22_Digital twin current scenario and a case st...   \n",
      "4  54_Revolutionizing Healthcare A Review Unveili...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Purpose â€“ The present paper aims to demonstrat...  \n",
      "1  been used predominantly for problems in engine...  \n",
      "2  Background: Digital breast tomosynthesis (DBT)...  \n",
      "3  In the current scenario, industries need to ha...  \n",
      "4  transformative force, holding the promise of r...  \n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_abstract_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "\n",
    "        lines = full_text.split('\\n')\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "        abstract = \"\"\n",
    "        is_abstract = False\n",
    "\n",
    "        for line in lines:\n",
    "            # Cerca inizio abstract\n",
    "            if \"abstract\" in line.lower():\n",
    "                is_abstract = True\n",
    "                continue\n",
    "\n",
    "            # Se parte abstract e si incontra \"1.\" (es. \"1. Introduction\") si interrompe\n",
    "            if is_abstract and (line.lower().startswith(\"1.\") or line.lower().startswith(\"keywords\")):\n",
    "                break\n",
    "\n",
    "            if is_abstract:\n",
    "                abstract += \" \" + line.strip()\n",
    "\n",
    "        return abstract.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore con {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_pdf_folder(folder_path):\n",
    "    papers = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            title_from_filename = filename.replace(\".pdf\", \"\")\n",
    "            abstract = extract_abstract_from_pdf(file_path)\n",
    "            if abstract:\n",
    "                papers.append({\n",
    "                    \"title\": title_from_filename,\n",
    "                    \"abstract\": abstract\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(papers)\n",
    "    return df\n",
    "\n",
    "# ESEMPIO: cambia questo con il path dove hai i tuoi PDF\n",
    "pdf_folder = \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/merged\"\n",
    "df_papers = process_pdf_folder(pdf_folder)\n",
    "\n",
    "# Visualizza e salva\n",
    "print(df_papers.head())\n",
    "df_papers.to_csv(\"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/index1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f82a33",
   "metadata": {},
   "source": [
    "### Now we start with the first query filtering ; \n",
    "- block1 = [\"digital twin\", \"virtual twin\"]\n",
    "- block2 = [\"healthcare\", \"medical\", \"clinical\", \"hospital\"]\n",
    "- block3 = [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\"]\n",
    "- block4 = [\"decision-making\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730453ad",
   "metadata": {},
   "source": [
    "**as**\n",
    "\n",
    "```(\"digital twin*\" OR \"virtual twin*\")  \n",
    "AND  \n",
    "(\"healthcare\" OR \"medical\" OR \"clinical\" OR \"hospital\")  \n",
    "AND  \n",
    "(\"surg*\" OR \"radiolog*\" OR \"oncol*\" OR \"cardiol*\" OR \"neurol*\" OR \"chronic disease*\")  \n",
    "AND  \n",
    "(\"decision-making\" OR \"clinical decision*\" OR \"diagnos*\" OR \"treatment planning\" OR \"patient management\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a00258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 1 - gruppi di keyword\n",
    "block1 = [\"digital twin\", \"virtual twin\"]\n",
    "block2 = [\"healthcare\", \"medical\", \"clinical\", \"hospital\"]\n",
    "block3 = [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\"]\n",
    "block4 = [\"decision-making\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\"]\n",
    "\n",
    "# Step 2 - funzione per match (wildcard con in operator)\n",
    "def matches_keywords(text, keywords):\n",
    "    return any(kw in text for kw in keywords)\n",
    "\n",
    "# Step 3 - funzione per filtrare il DataFrame \n",
    "def filter_relevant_papers(df):\n",
    "    filtered_papers = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        title = row[\"title\"].lower()\n",
    "        # Puoi anche aggiungere abstract con: title + \" \" + row[\"abstract\"].lower()\n",
    "\n",
    "        if (matches_keywords(title, block1) and\n",
    "            matches_keywords(title, block2) and\n",
    "            matches_keywords(title, block3) and\n",
    "            matches_keywords(title, block4)):\n",
    "            filtered_papers.append(row)\n",
    "\n",
    "    return pd.DataFrame(filtered_papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72dcbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Paper trovati: 0\n"
     ]
    }
   ],
   "source": [
    "# Assumendo che tu abbia giÃ  df_papers da prima\n",
    "df_filtered = filter_relevant_papers(df_papers)\n",
    "\n",
    "print(f\"ðŸ” Paper trovati: {len(df_filtered)}\")\n",
    "df_filtered.to_csv(\"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd1954",
   "metadata": {},
   "source": [
    "### no match for title , so below we try matching with same query but on abstract ;; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d10747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords divise in blocchi logici\n",
    "block1 = [\"digital twin\", \"virtual twin\"]\n",
    "block2 = [\"healthcare\", \"medical\", \"clinical\", \"hospital\"]\n",
    "block3 = [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\"]\n",
    "block4 = [\"decision-making\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\"]\n",
    "\n",
    "# Funzione per verificare se almeno una keyword Ã¨ contenuta nel testo\n",
    "def matches_keywords(text, keywords):\n",
    "    return any(kw in text for kw in keywords)\n",
    "\n",
    "# Funzione per filtrare i paper in base agli abstract\n",
    "def filter_relevant_papers(df):\n",
    "    filtered_papers = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        full_text = (row[\"abstract\"] or \"\").lower()\n",
    "\n",
    "        if (matches_keywords(full_text, block1) and\n",
    "            matches_keywords(full_text, block2) and\n",
    "            matches_keywords(full_text, block3) and\n",
    "            matches_keywords(full_text, block4)):\n",
    "            filtered_papers.append(row)\n",
    "\n",
    "    return pd.DataFrame(filtered_papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c318fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Paper selezionati (su abstract): 17\n"
     ]
    }
   ],
   "source": [
    "df_filtered = filter_relevant_papers(df_papers)\n",
    "\n",
    "print(f\"ðŸ“Œ Paper selezionati (su abstract): {len(df_filtered)}\")\n",
    "df_filtered.to_csv(\"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/abstract_parsing .csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12653ef7",
   "metadata": {},
   "source": [
    "#### Now just change query for filtering , :\n",
    "**as this**\n",
    "\n",
    "``` (\"digital twin*\" OR \"virtual twin*\" OR \"computational patient model*\" OR \"patient-specific simulation\")\n",
    "AND\n",
    "(\"healthcare\" OR \"medical\" OR \"clinical\" OR \"medicine\" OR \"hospital\")\n",
    "AND\n",
    "(\"surg*\" OR \"radiolog*\" OR \"oncol*\" OR \"cardiol*\" OR \"neurol*\" OR \"chronic disease*\" OR \"intensive care\" OR \"critical care\" OR \"therap*\")\n",
    "AND\n",
    "(\"decision-making\" OR \"decision support\" OR \"clinical decision*\" OR \"diagnos*\" OR \"treatment planning\" OR \"patient management\" OR \"predictive model*\" OR \"outcome prediction\") ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8af0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutti i filtri separati per blocchi logici, con wildcard gestita via \"startswith\"\n",
    "\n",
    "queries = [\n",
    "    {\n",
    "        \"label\": \"Query_1\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\", \"computational patient model\", \"patient-specific simulation\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\", \"medicine\", \"hospital\"],\n",
    "            [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\", \"intensive care\", \"critical care\", \"therap\"],\n",
    "            [\"decision-making\", \"decision support\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\", \"predictive model\", \"outcome prediction\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_2\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\"],\n",
    "            [\"application\", \"use case\", \"implementation\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_3\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin healthcare\"],\n",
    "            [\"virtual patient model clinical decision\"],\n",
    "            [\"medical digital twin treatment planning\"]\n",
    "        ],\n",
    "        \"phrase_mode\": True  # per cercare espressioni complete\n",
    "    }\n",
    "]\n",
    "\n",
    "# Funzione di supporto\n",
    "def matches_keywords_block(text, block):\n",
    "    return any(kw in text for kw in block)\n",
    "\n",
    "def matches_phrase_exact(text, phrase):\n",
    "    return phrase in text\n",
    "\n",
    "# Funzione per filtrare un DataFrame con le query strutturate\n",
    "def filter_papers_by_queries(df, queries):\n",
    "    df = df.copy()\n",
    "    df[\"full_text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        matched = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[\"full_text\"]\n",
    "\n",
    "            if query.get(\"phrase_mode\", False):\n",
    "                # Frasi complete (Query 3)\n",
    "                if any(matches_phrase_exact(text, phrase) for phrase in query[\"blocks\"][0]):\n",
    "                    matched.append(row)\n",
    "            else:\n",
    "                # Blocchi logici (Query 1 & 2)\n",
    "                if all(matches_keywords_block(text, block) for block in query[\"blocks\"]):\n",
    "                    matched.append(row)\n",
    "        \n",
    "        results[query[\"label\"]] = pd.DataFrame(matched)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58896fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------- STEP 1: Define the queries used for filtering -------------------\n",
    "# Three structured keyword-based queries were designed for title and abstract matching.\n",
    "queries = [\n",
    "    {\n",
    "        \"label\": \"Query_1\",\n",
    "        \"description\": \"Primary query combining digital twin concepts in healthcare and clinical decision-making.\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\", \"computational patient model\", \"patient-specific simulation\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\", \"medicine\", \"hospital\"],\n",
    "            [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\", \"intensive care\", \"critical care\", \"therap\"],\n",
    "            [\"decision-making\", \"decision support\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\", \"predictive model\", \"outcome prediction\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_2\",\n",
    "        \"description\": \"Query for general use of digital twin technologies in healthcare applications.\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\"],\n",
    "            [\"application\", \"use case\", \"implementation\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_3\",\n",
    "        \"description\": \"Phrase-based query to match complete thematic expressions.\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin healthcare\"],\n",
    "            [\"virtual patient model clinical decision\"],\n",
    "            [\"medical digital twin treatment planning\"]\n",
    "        ],\n",
    "        \"phrase_mode\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# ------------------- STEP 2: Define filtering functions -------------------\n",
    "def matches_keywords_block(text, block):\n",
    "    \"\"\"Check if any keyword from a block appears in the text.\"\"\"\n",
    "    return any(kw in text for kw in block)\n",
    "\n",
    "def matches_phrase_exact(text, phrase):\n",
    "    \"\"\"Check if a specific phrase appears in the text.\"\"\"\n",
    "    return phrase in text\n",
    "\n",
    "def filter_papers_by_queries(df, queries):\n",
    "    \"\"\"Apply each query to the dataset and return matching entries.\"\"\"\n",
    "    df = df.copy()  \n",
    "    df[\"full_text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        matched_rows = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[\"full_text\"]\n",
    "\n",
    "            if query.get(\"phrase_mode\", False):\n",
    "                # Phrase-based query (exact match)\n",
    "                if any(matches_phrase_exact(text, phrase) for phrase in query[\"blocks\"][0]):\n",
    "                    matched_rows.append(row)\n",
    "            else:\n",
    "                # Block-wise boolean AND query\n",
    "                if all(matches_keywords_block(text, block) for block in query[\"blocks\"]):\n",
    "                    matched_rows.append(row)\n",
    "\n",
    "        results[query[\"label\"]] = pd.DataFrame(matched_rows)\n",
    "        print(f\"ðŸ”Ž {query['label']}: {len(matched_rows)} papers matched | {query['description']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------- STEP 3: Run filtering over the DataFrame -------------------\n",
    "# Assumes df_papers is preloaded with 'title' and 'abstract' columns.\n",
    "filtered_results = filter_papers_by_queries(df_papers, queries)\n",
    "\n",
    "# ------------------- STEP 4: Save filtered results to local files -------------------\n",
    "# You can customize these paths to fit your machine or project folder structure.\n",
    "output_paths = {\n",
    "    \"Query_1\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered1.csv\",\n",
    "    \"Query_2\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered2.csv\",\n",
    "    \"Query_3\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered3.csv\"\n",
    "}\n",
    "\n",
    "# Ensure all output directories exist\n",
    "for path in output_paths.values():\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Save matched papers to their respective output files\n",
    "for label, df_result in filtered_results.items():\n",
    "    output_path = output_paths[label]\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Results saved for {label}: {len(df_result)} papers â†’ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29203f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query_1: 20 risultati\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'Query_1_filtered.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, df_result \u001b[38;5;129;01min\u001b[39;00m filtered\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m risultati\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mdf_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_filtered.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'Query_1_filtered.csv'"
     ]
    }
   ],
   "source": [
    "filtered = filter_papers_by_queries(df_papers, queries)\n",
    "\n",
    "# Salva ogni filtro in un CSV\n",
    "for label, df_result in filtered.items():\n",
    "    print(f\"{label}: {len(df_result)} risultati\")\n",
    "    df_result.to_csv(f\"{label}_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a64c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Query_1: 20 risultati salvati in\n",
      "âž¡ï¸ /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered1.csv\n",
      "âœ… Query_2: 41 risultati salvati in\n",
      "âž¡ï¸ /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered2.csv\n",
      "âœ… Query_3: 1 risultati salvati in\n",
      "âž¡ï¸ /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------- Step 1: Define queries -------------------\n",
    "queries = [\n",
    "    {\n",
    "        \"label\": \"Query_1\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\", \"computational patient model\", \"patient-specific simulation\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\", \"medicine\", \"hospital\"],\n",
    "            [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\", \"intensive care\", \"critical care\", \"therap\"],\n",
    "            [\"decision-making\", \"decision support\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\", \"predictive model\", \"outcome prediction\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_2\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\"],\n",
    "            [\"application\", \"use case\", \"implementation\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_3\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin healthcare\"],\n",
    "            [\"virtual patient model clinical decision\"],\n",
    "            [\"medical digital twin treatment planning\"]\n",
    "        ],\n",
    "        \"phrase_mode\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# ------------------- Step 2: Define matching functions -------------------\n",
    "def matches_keywords_block(text, block):\n",
    "    return any(kw in text for kw in block)\n",
    "\n",
    "def matches_phrase_exact(text, phrase):\n",
    "    return phrase in text\n",
    "\n",
    "def filter_papers_by_queries(df, queries):\n",
    "    df = df.copy()\n",
    "    df[\"full_text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        matched = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[\"full_text\"]\n",
    "\n",
    "            if query.get(\"phrase_mode\", False):\n",
    "                if any(matches_phrase_exact(text, phrase) for phrase in query[\"blocks\"][0]):\n",
    "                    matched.append(row)\n",
    "            else:\n",
    "                if all(matches_keywords_block(text, block) for block in query[\"blocks\"]):\n",
    "                    matched.append(row)\n",
    "        \n",
    "        results[query[\"label\"]] = pd.DataFrame(matched)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------- Step 3: Run filtering -------------------\n",
    "filtered_results = filter_papers_by_queries(df_papers, queries)\n",
    "\n",
    "# ------------------- Step 4: Define output paths -------------------\n",
    "output_paths = {\n",
    "    \"Query_1\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered1.csv\",\n",
    "    \"Query_2\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered2.csv\",\n",
    "    \"Query_3\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/new/filtered3.csv\"\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in output_paths.values():\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# ------------------- Step 5: Save filtered results -------------------\n",
    "for label, df_result in filtered_results.items():\n",
    "    output_path = output_paths[label]\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… {label}: {len(df_result)} risultati salvati in\\nâž¡ï¸ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1286b1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_papers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# ------------------- Step 3: Run filtering and export to CSV -------------------\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m filtered_results \u001b[38;5;241m=\u001b[39m filter_papers_by_queries(\u001b[43mdf_papers\u001b[49m, queries)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Salvataggio dei file CSV\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, df_result \u001b[38;5;129;01min\u001b[39;00m filtered_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_papers' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------- Step 1: Define queries -------------------\n",
    "\n",
    "queries = [\n",
    "    {\n",
    "        \"label\": \"Query_1\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\", \"computational patient model\", \"patient-specific simulation\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\", \"medicine\", \"hospital\"],\n",
    "            [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\", \"intensive care\", \"critical care\", \"therap\"],\n",
    "            [\"decision-making\", \"decision support\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\", \"predictive model\", \"outcome prediction\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_2\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\"],\n",
    "            [\"application\", \"use case\", \"implementation\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_3\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin healthcare\"],\n",
    "            [\"virtual patient model clinical decision\"],\n",
    "            [\"medical digital twin treatment planning\"]\n",
    "        ],\n",
    "        \"phrase_mode\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# ------------------- Step 2: Define matching functions -------------------\n",
    "\n",
    "def matches_keywords_block(text, block):\n",
    "    return any(kw in text for kw in block)\n",
    "\n",
    "def matches_phrase_exact(text, phrase):\n",
    "    return phrase in text\n",
    "\n",
    "def filter_papers_by_queries(df, queries):\n",
    "    df = df.copy()\n",
    "    df[\"full_text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        matched = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[\"full_text\"]\n",
    "\n",
    "            if query.get(\"phrase_mode\", False):\n",
    "                if any(matches_phrase_exact(text, phrase) for phrase in query[\"blocks\"][0]):\n",
    "                    matched.append(row)\n",
    "            else:\n",
    "                if all(matches_keywords_block(text, block) for block in query[\"blocks\"]):\n",
    "                    matched.append(row)\n",
    "        \n",
    "        results[query[\"label\"]] = pd.DataFrame(matched)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------- Step 3: Run filtering and export to CSV -------------------\n",
    "\n",
    "filtered_results = filter_papers_by_queries(df_papers, queries)\n",
    "\n",
    "# Salvataggio dei file CSV\n",
    "for label, df_result in filtered_results.items():\n",
    "    print(f\"ðŸ”Ž {label}: {len(df_result)} paper trovati.\")\n",
    "    df_result.to_csv(f\"{label}_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d7cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Query_1: 20 papers matched | Primary query combining digital twin concepts in healthcare and clinical decision-making.\n",
      "ðŸ”Ž Query_2: 41 papers matched | Query for general use of digital twin technologies in healthcare applications.\n",
      "ðŸ”Ž Query_3: 1 papers matched | Phrase-based query to match complete thematic expressions.\n",
      "âœ… Results saved for Query_1: 20 papers â†’ /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/parsed/query1.csv\n",
      "âœ… Results saved for Query_2: 41 papers â†’ /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/parsed/query2.csv\n",
      "âœ… Results saved for Query_3: 1 papers â†’ /Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/parsed/query3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_papers = pd.read_csv('/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/index1.csv')\n",
    "\n",
    "# ------------------- STEP 1: Define the queries used for filtering -------------------\n",
    "# Three structured keyword-based queries were designed for title and abstract matching.\n",
    "queries = [\n",
    "    {\n",
    "        \"label\": \"Query_1\",\n",
    "        \"description\": \"Primary query combining digital twin concepts in healthcare and clinical decision-making.\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\", \"computational patient model\", \"patient-specific simulation\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\", \"medicine\", \"hospital\"],\n",
    "            [\"surg\", \"radiolog\", \"oncol\", \"cardiol\", \"neurol\", \"chronic disease\", \"intensive care\", \"critical care\", \"therap\"],\n",
    "            [\"decision-making\", \"decision support\", \"clinical decision\", \"diagnos\", \"treatment planning\", \"patient management\", \"predictive model\", \"outcome prediction\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_2\",\n",
    "        \"description\": \"Query for general use of digital twin technologies in healthcare applications.\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin\", \"virtual twin\"],\n",
    "            [\"healthcare\", \"medical\", \"clinical\"],\n",
    "            [\"application\", \"use case\", \"implementation\"]\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Query_3\",\n",
    "        \"description\": \"Phrase-based query to match complete thematic expressions.\",\n",
    "        \"blocks\": [\n",
    "            [\"digital twin healthcare\"],\n",
    "            [\"virtual patient model clinical decision\"],\n",
    "            [\"medical digital twin treatment planning\"]\n",
    "        ],\n",
    "        \"phrase_mode\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# ------------------- STEP 2: Define filtering functions -------------------\n",
    "def matches_keywords_block(text, block):\n",
    "    \"\"\"Check if any keyword from a block appears in the text.\"\"\"\n",
    "    return any(kw in text for kw in block)\n",
    "\n",
    "def matches_phrase_exact(text, phrase):\n",
    "    \"\"\"Check if a specific phrase appears in the text.\"\"\"\n",
    "    return phrase in text\n",
    "\n",
    "def filter_papers_by_queries(df, queries):\n",
    "    \"\"\"Apply each query to the dataset and return matching entries.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"full_text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        matched_rows = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[\"full_text\"]\n",
    "\n",
    "            if query.get(\"phrase_mode\", False):\n",
    "                # Phrase-based query (exact match)\n",
    "                if any(matches_phrase_exact(text, phrase) for phrase in query[\"blocks\"][0]):\n",
    "                    matched_rows.append(row)\n",
    "            else:\n",
    "                # Block-wise boolean AND query\n",
    "                if all(matches_keywords_block(text, block) for block in query[\"blocks\"]):\n",
    "                    matched_rows.append(row)\n",
    "\n",
    "        results[query[\"label\"]] = pd.DataFrame(matched_rows)\n",
    "        print(f\"ðŸ”Ž {query['label']}: {len(matched_rows)} papers matched | {query['description']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ------------------- STEP 3: Run filtering over the DataFrame -------------------\n",
    "# Assumes df_papers is preloaded with 'title' and 'abstract' columns.\n",
    "\n",
    "filtered_results = filter_papers_by_queries(df_papers, queries)\n",
    "\n",
    "# ------------------- STEP 4: Save filtered results to local files -------------------\n",
    "# You can customize these paths to fit your machine or project folder structure.\n",
    "output_paths = {\n",
    "    \"Query_1\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/parsed/query1.csv\",\n",
    "    \"Query_2\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/parsed/query2.csv\",\n",
    "    \"Query_3\": \"/Users/roberto/Desktop/UNI_DATASCIENCE/QME/HM1/paper_1/parsed/query3.csv\"\n",
    "}\n",
    "\n",
    "# Ensure all output directories exist\n",
    "for path in output_paths.values():\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Save matched papers to their respective output files\n",
    "for label, df_result in filtered_results.items():\n",
    "    output_path = output_paths[label]\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Results saved for {label}: {len(df_result)} papers â†’ {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
